# TRAIN ON MASSIVE COMBINED DATASET - 3,329+ SAMPLES!
# Combine original + reference + video frames for breakthrough results

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import numpy as np
import cv2
import os
import glob
from tqdm import tqdm
from sklearn.model_selection import train_test_split

class MassiveDataset(Dataset):
    def _init_(self, df, transform=None):
        self.df = df
        self.transform = transform
    
    def _len_(self):
        return len(self.df)
    
    def _getitem_(self, idx):
        row = self.df.iloc[idx]
        img_path = row['full_path']
        
        try:
            image = cv2.imread(img_path)
            if image is None:
                # Create black image if loading fails
                image = np.zeros((224, 224, 3), dtype=np.uint8)
            else:
                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                
            image = Image.fromarray(image)
            
            if self.transform:
                image = self.transform(image)
            
            return image, row['label_idx']
        except:
            # Return black image on any error
            black_image = torch.zeros(3, 224, 224, dtype=torch.float32)
            return black_image, row['label_idx']

class BreakthroughModel(nn.Module):
    def _init_(self, num_classes):
        super(BreakthroughModel, self)._init_()
        
        # Use ResNet50 for reliability
        self.backbone = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)
        
        # Enhanced classifier for massive dataset
        self.backbone.fc = nn.Sequential(
            nn.Dropout(0.4),
            nn.Linear(2048, 768),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(768, num_classes)
        )
    
    def forward(self, x):
        return self.backbone(x)

def create_massive_combined_dataset():
    """Combine ALL data sources into one massive dataset"""
    print("üöÄ CREATING MASSIVE COMBINED DATASET")
    print("="*60)
    
    all_data = []
    
    # 1. Original training data
    print("1Ô∏è‚É£ Adding original training data...")
    train_df = pd.read_csv("/kaggle/input/identity-employees-in-surveillance-cctv/dataset/train/labels.csv")
    train_img_dir = "/kaggle/input/identity-employees-in-surveillance-cctv/dataset/train/images/"
    
    for _, row in train_df.iterrows():
        all_data.append({
            'filename': row['filename'],
            'emp_id': row['emp_id'],
            'full_path': os.path.join(train_img_dir, row['filename']),
            'source': 'original_train'
        })
    
    print(f"   ‚úÖ Original training: {len(train_df)} samples")
    
    # 2. Reference face images
    print("2Ô∏è‚É£ Adding reference face images...")
    reference_faces_dir = "/kaggle/input/identity-employees-in-surveillance-cctv/dataset/reference_faces"
    ref_count = 0
    
    for emp_id in os.listdir(reference_faces_dir):
        emp_dir = os.path.join(reference_faces_dir, emp_id)
        if os.path.isdir(emp_dir) and emp_id.startswith('emp'):
            # Find JPG files (skip MP4 videos)
            jpg_files = glob.glob(os.path.join(emp_dir, "*.jpg"))
            for jpg_path in jpg_files:
                jpg_name = os.path.basename(jpg_path)
                all_data.append({
                    'filename': jpg_name,
                    'emp_id': emp_id,
                    'full_path': jpg_path,
                    'source': 'reference_image'
                })
                ref_count += 1
    
    print(f"   ‚úÖ Reference images: {ref_count} samples")
    
    # 3. Extracted video frames
    print("3Ô∏è‚É£ Adding extracted video frames...")
    video_count = 0
    if os.path.exists('massive_video_frames_labels.csv'):
        video_df = pd.read_csv('massive_video_frames_labels.csv')
        
        for _, row in video_df.iterrows():
            all_data.append({
                'filename': row['filename'],
                'emp_id': row['emp_id'],
                'full_path': row['full_path'],
                'source': 'video_frame'
            })
            video_count += 1
        
        print(f"   ‚úÖ Video frames: {video_count} samples")
    else:
        print("   ‚ö†Ô∏è Video frames CSV not found")
    
    # Create massive DataFrame
    massive_df = pd.DataFrame(all_data)
    
    # Clean and validate
    massive_df = massive_df[massive_df['emp_id'].str.startswith('emp')]
    massive_df = massive_df.dropna()
    
    print(f"\nüéâ MASSIVE DATASET CREATED!")
    print(f"‚úÖ Total samples: {len(massive_df)}")
    print(f"‚úÖ Unique employees: {massive_df['emp_id'].nunique()}")
    print(f"‚úÖ Sources breakdown:")
    for source in massive_df['source'].unique():
        count = len(massive_df[massive_df['source'] == source])
        print(f"      {source}: {count} samples")
    
    return massive_df

def train_on_massive_dataset():
    """Train model on the massive combined dataset"""
    print("üöÄ TRAINING ON MASSIVE DATASET")
    print("="*60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Device: {device}")
    
    # Create massive dataset
    massive_df = create_massive_combined_dataset()
    
    if len(massive_df) == 0:
        print("‚ùå No data found!")
        return
    
    # Create label mapping
    unique_employees = sorted(massive_df['emp_id'].unique())
    class_to_idx = {emp: idx for idx, emp in enumerate(unique_employees)}
    idx_to_class = {idx: emp for emp, idx in class_to_idx.items()}
    
    # Add label indices
    massive_df['label_idx'] = massive_df['emp_id'].map(class_to_idx)
    
    print(f"üìä Dataset Overview:")
    print(f"   Classes: {len(unique_employees)}")
    print(f"   Samples per employee: {len(massive_df) / len(unique_employees):.1f} avg")
    
    # Enhanced transforms for massive dataset
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop((224, 224)),
        transforms.RandomHorizontalFlip(0.5),
        transforms.RandomRotation(15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Train/val split
    train_split, val_split = train_test_split(
        massive_df, 
        test_size=0.15,  # Smaller val set since we have massive data
        random_state=42,
        stratify=massive_df['emp_id']  # Stratify by employee
    )
    
    # Create datasets
    train_dataset = MassiveDataset(train_split, train_transform)
    val_dataset = MassiveDataset(val_split, val_transform)
    
    # Data loaders
    train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=24, shuffle=False, num_workers=2)
    
    print(f"üìä Split Overview:")
    print(f"   Train samples: {len(train_split)}")
    print(f"   Val samples: {len(val_split)}")
    
    # Model
    model = BreakthroughModel(len(unique_employees)).to(device)
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30)
    
    # Training parameters
    num_epochs = 30
    best_val_acc = 0
    stagnation_count = 0
    stagnation_threshold = 6
    
    train_losses = []
    val_losses = []
    val_accuracies = []
    
    print(f"\nüöÄ MASSIVE TRAINING EXPECTATIONS:")
    print(f"   Previous best: ~75% validation")
    print(f"   Dataset boost: 1,179 ‚Üí {len(massive_df)} samples (2.8x)")
    print(f"   Expected validation: 80-85%+ easily!")
    print(f"   Expected competition: 0.75-0.85 range!")
    
    for epoch in range(num_epochs):
        # TRAINING
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.1f}%'
            })
        
        scheduler.step()
        
        # VALIDATION
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        # Calculate metrics
        epoch_train_loss = train_loss / len(train_loader)
        epoch_val_loss = val_loss / len(val_loader)
        epoch_val_acc = 100. * val_correct / val_total
        epoch_train_acc = 100. * train_correct / train_total
        
        train_losses.append(epoch_train_loss)
        val_losses.append(epoch_val_loss)
        val_accuracies.append(epoch_val_acc)
        
        print(f"\nüìä Epoch {epoch+1}/{num_epochs}:")
        print(f"   Train: Loss {epoch_train_loss:.4f}, Acc {epoch_train_acc:.2f}%")
        print(f"   Val: Loss {epoch_val_loss:.4f}, Acc {epoch_val_acc:.2f}%")
        print(f"   LR: {scheduler.get_last_lr()[0]:.6f}")
        
        # BREAKTHROUGH MILESTONE TRACKING
        if epoch_val_acc >= 85.0:
            print(f"   üî•üî•üî• BREAKTHROUGH! 85%+ - Competition score likely 0.85+!")
        elif epoch_val_acc >= 82.0:
            print(f"   üî•üî• INCREDIBLE! 82%+ - Competition score likely 0.80+!")
        elif epoch_val_acc >= 80.0:
            print(f"   üî• AMAZING! 80%+ - Competition score likely 0.75+!")
        elif epoch_val_acc >= 78.0:
            print(f"   üöÄ EXCELLENT! 78%+ - Major improvement!")
        elif epoch_val_acc >= 76.0:
            print(f"   ‚úÖ GREAT! 76%+ - Solid boost!")
        elif epoch_val_acc >= 75.0:
            print(f"   üìà GOOD! 75%+ - Moving up!")
        
        # Save best model
        if epoch_val_acc > best_val_acc:
            best_val_acc = epoch_val_acc
            stagnation_count = 0
            
            torch.save({
                'model_state_dict': model.state_dict(),
                'class_mapping': idx_to_class,
                'train_accuracy': epoch_train_acc,
                'val_accuracy': best_val_acc,
                'epoch': epoch,
                'dataset_size': len(massive_df),
                'num_classes': len(unique_employees)
            }, 'massive_breakthrough_model.pth')
            
            print(f"   ‚úÖ NEW BEST! Saved massive_breakthrough_model.pth")
            
        else:
            stagnation_count += 1
            print(f"   ‚ö†Ô∏è Stagnated for {stagnation_count} epochs")
        
        if stagnation_count >= stagnation_threshold:
            print(f"üõë Early stopping after {stagnation_threshold} epochs of stagnation")
            break
    
    print(f"\nüéâ MASSIVE TRAINING COMPLETE!")
    print(f"üèÜ Best validation accuracy: {best_val_acc:.2f}%")
    print(f"üìä Dataset size: {len(massive_df)} samples")
    print(f"üìà Improvement over 1,179 samples: MASSIVE!")
    
    # Performance prediction
    if best_val_acc >= 80.0:
        print(f"üéä BREAKTHROUGH ACHIEVED! 80%+ validation!")
        print(f"üèÜ Expected competition score: 0.75-0.85 range!")
        print(f"üöÄ You're now in TOP TIER performance!")
    elif best_val_acc >= 78.0:
        print(f"üî• EXCELLENT RESULTS! Major improvement!")
        print(f"üéØ Expected competition score: 0.72-0.78 range!")
    elif best_val_acc >= 76.0:
        print(f"‚úÖ SOLID IMPROVEMENT! Good boost!")
        print(f"üìà Expected competition score: 0.70-0.75 range!")
    
    return model, idx_to_class, best_val_acc

def create_massive_submission(model_path='massive_breakthrough_model.pth'):
    """Create submission with massive model"""
    print("üöÄ CREATING MASSIVE MODEL SUBMISSION")
    print("="*50)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # Load model
    checkpoint = torch.load(model_path, map_location=device)
    class_mapping = checkpoint['class_mapping']
    num_classes = len(class_mapping)
    
    model = BreakthroughModel(num_classes).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"‚úÖ Loaded massive model:")
    print(f"   Classes: {num_classes}")
    print(f"   Validation accuracy: {checkpoint['val_accuracy']:.2f}%")
    print(f"   Dataset size: {checkpoint.get('dataset_size', 'Unknown')}")
    
    # Test transform
    test_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # Get test images
    test_dir = "/kaggle/input/identity-employees-in-surveillance-cctv/dataset/test/images"
    test_images = glob.glob(os.path.join(test_dir, "*.jpg"))
    
    print(f"Found {len(test_images)} test images")
    
    # Try multiple thresholds for best results
    thresholds = [0.25, 0.30, 0.35, 0.40]
    
    for threshold in thresholds:
        results = []
        
        with torch.no_grad():
            for img_path in tqdm(test_images, desc=f"Processing (th={threshold})"):
                img_name = os.path.basename(img_path)
                
                try:
                    image = cv2.imread(img_path)
                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                    image = Image.fromarray(image)
                    image = test_transform(image)
                    image = image.unsqueeze(0).to(device)
                    
                    outputs = model(image)
                    probabilities = torch.softmax(outputs, dim=1)
                    max_prob, predicted_idx = torch.max(probabilities, 1)
                    
                    if max_prob.item() >= threshold:
                        predicted_emp_id = class_mapping[predicted_idx.item()]
                    else:
                        predicted_emp_id = "unknown"
                    
                    results.append({
                        'filename': img_name,
                        'employee_id': predicted_emp_id
                    })
                    
                except Exception as e:
                    results.append({
                        'filename': img_name,
                        'employee_id': "unknown"
                    })
        
        # Save submission
        submission_df = pd.DataFrame(results)
        filename = f'massive_submission_th{threshold:.2f}.csv'
        submission_df.to_csv(filename, index=False)
        
        unknown_count = len(submission_df[submission_df['employee_id'] == 'unknown'])
        print(f"‚úÖ {filename}: {unknown_count} unknowns ({100*unknown_count/len(submission_df):.1f}%)")
    
    print(f"\nüéØ MASSIVE SUBMISSIONS READY!")
    print(f"üèÜ Expected HUGE improvement over previous 0.69 score!")

# MAIN EXECUTION
if _name_ == "_main_":
    print("üöÄ TRAINING ON MASSIVE 3,329+ SAMPLE DATASET")
    print("This should give you BREAKTHROUGH results!")
    
    # Train on massive dataset
    model, class_mapping, best_acc = train_on_massive_dataset()
    
    # Create submissions
    create_massive_submission()
    
    print(f"\nüéâ BREAKTHROUGH TRAINING COMPLETE!")
    print(f"üèÜ Best accuracy: {best_acc:.2f}%")
    print(f"üöÄ Expected massive competition improvement!")
